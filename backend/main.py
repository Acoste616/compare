import asyncio
import json
import time
import logging
import uuid
from pathlib import Path
from typing import List, Optional, Dict

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Depends, HTTPException, BackgroundTasks, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from sqlalchemy.orm import selectinload
from pydantic import BaseModel
from qdrant_client.http import models

# Imports from your project structure
from backend.database import init_db, get_db, AsyncSessionLocal
from backend.models import Session as DBSession, Message as DBMessage, AnalysisState as DBAnalysisState, FeedbackLog
from backend.rag_engine import rag_engine
from backend.ai_core import ai_core, create_emergency_response
from backend.analysis_engine import analysis_engine, SystemBusyException
from backend.gotham_module import GothamIntelligence, BurningHouseInput, BurningHouseCalculator, CEPiKConnector, CEPiKData, SniperGateway
from backend.dojo_refiner import dojo_refiner

# V5.0: Import UnifiedPipeline (replaces sniper_module)
from asset_sniper.unified_platform import UnifiedPipeline, PipelineConfig, PipelineStats, ProcessingLevel
from asset_sniper.config import GOLDEN_CITY_M2_PRICES, Tier

# Setup Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI()

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# === MODELS ===

class RAGNuggetEdit(BaseModel):
    title: str
    content: str
    keywords: List[str]
    language: str

class GoldenStandardAdd(BaseModel):
    trigger_context: str
    golden_response: str
    category: str
    language: str = "PL"


class FeedbackSubmit(BaseModel):
    """Schema for submitting feedback from the frontend"""
    session_id: Optional[str] = None
    module_name: str  # e.g., "fast_path", "slow_path_m1_dna"
    rating: bool  # True = Like, False = Dislike
    user_input_snapshot: Optional[str] = None  # Input context (user message / session summary)
    ai_output_snapshot: Optional[str] = None   # AI output being rated
    expert_comment: Optional[str] = None       # Expert's correction
    message_id: Optional[str] = None

class GothamScoreRequest(BaseModel):
    """Schema for GOTHAM Burning House score calculation"""
    monthly_fuel_cost: float
    current_car_value: float
    annual_tax: float = 225_000
    has_family_card: bool = False
    region: str = "≈öLƒÑSKIE"

class GothamMarketUpdate(BaseModel):
    """Schema for updating GOTHAM market data (Admin Panel)"""
    region: str
    total_ev_registrations: int
    growth_rate: Optional[float] = None
    top_brand: Optional[str] = None
    trend: Optional[str] = None


# === V5.0: ASYNC JOB STORAGE FOR SNIPER PIPELINE ===
# In-memory storage for job status (use Redis in production)
SNIPER_JOBS: Dict[str, Dict] = {}


class SniperJobStatus:
    """Job status constants"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"


def parse_xml_to_dataframe_chunks(xml_path: str, chunk_size: int = 10000):
    """
    Parse CEIDG XML file iteratively using lxml.etree.iterparse.

    Yields DataFrames in chunks to avoid loading entire XML into memory.

    XML Structure (CEIDG):
    <root>
        <dane>
            <NIP>5272829917</NIP>
            <Nazwa>Firma Sp. z o.o.</Nazwa>
            <Imie>Jan</Imie>
            <Nazwisko>Kowalski</Nazwisko>
            <GlownyKodPkd>6201Z</GlownyKodPkd>
            <Miejscowosc>Warszawa</Miejscowosc>
            <KodPocztowy>00-001</KodPocztowy>
            <Wojewodztwo>MAZOWIECKIE</Wojewodztwo>
            <Telefon>500100200</Telefon>
            <Email>kontakt@firma.pl</Email>
            <DataRozpoczeciaDzialalnosci>2019-03-15</DataRozpoczeciaDzialalnosci>
            <FormaPrawna>OSOBA FIZYCZNA PROWADZƒÑCA DZIA≈ÅALNO≈öƒÜ GOSPODARCZƒÑ</FormaPrawna>
            <StatusDzialalnosci>AKTYWNA</StatusDzialalnosci>
        </dane>
        ...
    </root>

    Args:
        xml_path: Path to XML file
        chunk_size: Number of records per chunk

    Yields:
        pd.DataFrame: Chunks of parsed data
    """
    try:
        from lxml import etree
    except ImportError:
        raise ImportError("lxml is required for XML parsing. Install with: pip install lxml")

    import pandas as pd

    # XML tag to DataFrame column mapping (CEIDG format)
    tag_mapping = {
        'NIP': 'nip',
        'Nazwa': 'nazwa',
        'NazwaPodmiotu': 'nazwa',
        'NazwaSkrocona': 'nazwa',
        'Imie': 'first_name',
        'Nazwisko': 'last_name',
        'GlownyKodPkd': 'pkd',
        'G≈Ç√≥wnyKodPKD': 'pkd',
        'PkdGlowny': 'pkd',
        'Miejscowosc': 'city',
        'Miasto': 'city',
        'KodPocztowy': 'zip_code',
        'Wojewodztwo': 'voivodeship',
        'Telefon': 'phone',
        'Email': 'email',
        'DataRozpoczeciaDzialalnosci': 'start_date',
        'FormaPrawna': 'legal_form',
        'StatusDzialalnosci': 'status'
    }

    records = []
    count = 0

    logger.info(f"[XML PARSER] Starting iterative parsing of {xml_path}")

    # Use iterparse for memory-efficient parsing
    context = etree.iterparse(xml_path, events=('end',), tag='dane')

    for event, elem in context:
        # Extract data from current <dane> element
        record = {}

        for child in elem:
            tag = child.tag
            value = child.text

            # Map XML tag to our internal field name
            if tag in tag_mapping:
                field_name = tag_mapping[tag]
                record[field_name] = value if value else ''

        if record:  # Only add non-empty records
            records.append(record)
            count += 1

        # Clear element to free memory
        elem.clear()
        while elem.getprevious() is not None:
            del elem.getparent()[0]

        # Yield chunk when size reached
        if len(records) >= chunk_size:
            logger.info(f"[XML PARSER] Yielding chunk with {len(records)} records (Total processed: {count})")
            yield pd.DataFrame(records)
            records = []

    # Yield remaining records
    if records:
        logger.info(f"[XML PARSER] Yielding final chunk with {len(records)} records")
        yield pd.DataFrame(records)

    logger.info(f"[XML PARSER] ‚úÖ Parsing complete. Total records: {count}")


def parse_csv_to_dataframe_chunks(csv_path: str, chunk_size: int = 10000):
    """
    Parse CSV file in chunks using pandas.

    Tries multiple encodings for Polish CSV files.

    Args:
        csv_path: Path to CSV file
        chunk_size: Number of rows per chunk

    Yields:
        pd.DataFrame: Chunks of parsed data
    """
    import pandas as pd

    logger.info(f"[CSV PARSER] Starting chunked parsing of {csv_path}")

    # Try different encodings (common for Polish files)
    encodings = ['utf-8', 'cp1250', 'iso-8859-2', 'latin1']

    for encoding in encodings:
        try:
            chunk_iterator = pd.read_csv(
                csv_path,
                chunksize=chunk_size,
                encoding=encoding,
                sep=None,  # Auto-detect separator
                engine='python',
                on_bad_lines='skip'
            )

            logger.info(f"[CSV PARSER] Successfully opened CSV with encoding: {encoding}")

            chunk_num = 0
            for chunk_df in chunk_iterator:
                chunk_num += 1
                logger.info(f"[CSV PARSER] Yielding chunk {chunk_num} with {len(chunk_df)} rows")
                yield chunk_df

            logger.info(f"[CSV PARSER] ‚úÖ Parsing complete. Total chunks: {chunk_num}")
            return

        except Exception as e:
            logger.debug(f"[CSV PARSER] Failed with encoding {encoding}: {e}")
            continue

    raise ValueError(f"Could not parse CSV file with any of these encodings: {encodings}")


async def process_file_job(job_id: str, file_path: str, file_type: str, enable_deep: bool):
    """
    Background task to process CSV or XML file through UnifiedPipeline using streaming.

    This version processes files in chunks to handle 100MB+ files without memory issues.

    Updates SNIPER_JOBS with progress and results.

    Args:
        job_id: Unique job identifier
        file_path: Path to temporary file on disk
        file_type: File extension ('.csv' or '.xml')
        enable_deep: Whether to enable BigDecoder deep enrichment
    """
    import io
    import time

    try:
        SNIPER_JOBS[job_id]["status"] = SniperJobStatus.PROCESSING
        SNIPER_JOBS[job_id]["progress"] = 5
        logger.info(f"[SNIPER JOB {job_id}] Starting streaming processing...")

        # Select appropriate chunk parser based on file type
        if file_type == '.xml':
            chunk_generator = parse_xml_to_dataframe_chunks(file_path, chunk_size=10000)
        elif file_type == '.csv':
            chunk_generator = parse_csv_to_dataframe_chunks(file_path, chunk_size=10000)
        else:
            raise ValueError(f"Unsupported file type: {file_type}")

        SNIPER_JOBS[job_id]["progress"] = 10

        # Initialize UnifiedPipeline
        config = PipelineConfig(
            enable_gotham=True,
            enable_bigdecoder=enable_deep,
            bigdecoder_tier_threshold="AAA" if enable_deep else "S"
        )

        pipeline = UnifiedPipeline(
            config=config,
            analysis_engine=analysis_engine if enable_deep else None
        )

        SNIPER_JOBS[job_id]["progress"] = 15
        logger.info(f"[SNIPER JOB {job_id}] Pipeline initialized")

        # Process chunks and accumulate results
        import pandas as pd
        from asset_sniper.lead_refinery import LeadRefinery

        refinery = LeadRefinery()
        level = ProcessingLevel.LEVEL_3_BIGDECODER if enable_deep else ProcessingLevel.LEVEL_2_GOTHAM

        # Accumulate stats without keeping full DataFrames in memory
        total_rows_raw = 0
        total_rows_cleaned = 0
        total_rows_enriched = 0
        chunk_count = 0

        # Stats accumulators
        tier_counts = {}
        wealth_scores_sum = 0
        total_scores_sum = 0
        enriched_count = 0

        # Create temporary output file for streaming results
        import tempfile
        output_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv', encoding='utf-8')
        output_path = output_file.name
        first_chunk = True

        try:
            for chunk_df in chunk_generator:
                chunk_count += 1
                chunk_size = len(chunk_df)
                total_rows_raw += chunk_size

                logger.info(f"[SNIPER JOB {job_id}] Processing chunk {chunk_count} ({chunk_size} rows)")

                # Update total rows in job status
                SNIPER_JOBS[job_id]["total_rows"] = total_rows_raw

                # Level 0: Clean data with LeadRefinery
                chunk_clean = refinery.refine(chunk_df, require_phone=False, require_email=False)
                total_rows_cleaned += len(chunk_clean)

                if len(chunk_clean) == 0:
                    logger.warning(f"[SNIPER JOB {job_id}] Chunk {chunk_count} produced no clean rows, skipping...")
                    continue

                # Process through UnifiedPipeline
                chunk_enriched, chunk_stats = await pipeline.process(chunk_clean, level=level)
                total_rows_enriched += len(chunk_enriched)

                # Write chunk to output file (streaming)
                chunk_enriched.to_csv(
                    output_path,
                    mode='w' if first_chunk else 'a',
                    header=first_chunk,
                    index=False,
                    encoding='utf-8'
                )
                first_chunk = False

                # Accumulate stats (memory-efficient - just numbers, not DataFrames)
                if 'target_tier' in chunk_enriched.columns:
                    chunk_tier_counts = chunk_enriched['target_tier'].value_counts().to_dict()
                    for tier, count in chunk_tier_counts.items():
                        tier_counts[tier] = tier_counts.get(tier, 0) + count

                if 'wealth_score' in chunk_enriched.columns:
                    wealth_scores_sum += chunk_enriched['wealth_score'].sum()

                if 'total_score' in chunk_enriched.columns:
                    total_scores_sum += chunk_enriched['total_score'].sum()
                    enriched_count += len(chunk_enriched)

                # Update progress (5-80% range for processing)
                progress = min(80, 15 + int((chunk_count / max(1, total_rows_raw / 10000)) * 65))
                SNIPER_JOBS[job_id]["progress"] = progress

                logger.info(f"[SNIPER JOB {job_id}] ‚úì Chunk {chunk_count} complete: {len(chunk_enriched)} rows enriched")

            output_file.close()

            SNIPER_JOBS[job_id]["progress"] = 85

            # Calculate final stats from accumulated data
            if total_rows_enriched > 0:
                avg_wealth = wealth_scores_sum / enriched_count if enriched_count > 0 else 0
                avg_total = total_scores_sum / enriched_count if enriched_count > 0 else 0

                # Read final CSV output
                with open(output_path, 'r', encoding='utf-8') as f:
                    result_csv = f.read()

                # Store results
                SNIPER_JOBS[job_id]["status"] = SniperJobStatus.COMPLETED
                SNIPER_JOBS[job_id]["progress"] = 100
                SNIPER_JOBS[job_id]["result_csv"] = result_csv
                SNIPER_JOBS[job_id]["stats"] = {
                    "total_rows": total_rows_raw,
                    "cleaned_rows": total_rows_cleaned,
                    "enriched_rows": total_rows_enriched,
                    "scored_rows": total_rows_enriched,
                    "tier_counts": tier_counts,
                    "avg_wealth_score": float(avg_wealth),
                    "avg_total_score": float(avg_total),
                    "processing_time_ms": int((time.time() - SNIPER_JOBS[job_id]["created_at"] / 1000) * 1000),
                    "chunks_processed": chunk_count,
                    "file_type": file_type
                }
                SNIPER_JOBS[job_id]["completed_at"] = int(time.time() * 1000)

                logger.info(f"[SNIPER JOB {job_id}] ‚úÖ Completed! {total_rows_raw} rows processed in {chunk_count} chunks")

            else:
                raise ValueError("No data chunks were successfully processed")

        finally:
            # Cleanup temporary files
            Path(output_path).unlink(missing_ok=True)
            Path(file_path).unlink(missing_ok=True)

    except Exception as e:
        logger.error(f"[SNIPER JOB {job_id}] ‚ùå Failed: {e}")
        import traceback
        traceback.print_exc()

        SNIPER_JOBS[job_id]["status"] = SniperJobStatus.FAILED
        SNIPER_JOBS[job_id]["error"] = str(e)

        # Cleanup temp file on error
        try:
            Path(file_path).unlink(missing_ok=True)
        except:
            pass


@app.on_event("startup")
async def on_startup():
    await init_db()
    print("[DB] OK - Database initialized")

    # Load custom GOTHAM market data (if available)
    CEPiKConnector.load_custom_data()
    print("[GOTHAM] OK - Market data loaded")

    # Initialize fuel price scraper (preload with fresh data)
    try:
        from backend.services.gotham.scraper import FuelPriceScraper
        prices = FuelPriceScraper.get_prices_with_cache(force_refresh=False)
        print(f"[GOTHAM] Fuel Prices Loaded: Pb95={prices.get('Pb95', 6.05)} PLN, ON={prices.get('ON', 6.15)} PLN, LPG={prices.get('LPG', 2.85)} PLN")
    except Exception as e:
        print(f"[GOTHAM] WARNING - Fuel scraper initialization failed: {e}")

# === ADMIN ENDPOINTS ===

@app.get("/api/admin/rag/list")
async def list_rag_nuggets():
    """List all RAG nuggets (Proxy to engine)."""
    try:
        return {"nuggets": rag_engine.nuggets if hasattr(rag_engine, 'nuggets') else [], "total": 0}
    except:
        return {"nuggets": [], "total": 0}

@app.put("/api/admin/rag/edit/{nugget_id}")
async def edit_rag_nugget(nugget_id: str, edit_data: RAGNuggetEdit):
    """
    Edit existing RAG nugget in Qdrant.

    Steps:
    1. Find point in Qdrant by ID
    2. Update payload with new content
    3. Optionally recalculate embedding if content changed
    """
    try:
        if not rag_engine.client or not rag_engine.model:
            raise HTTPException(status_code=503, detail="RAG engine not available")

        # 1. Search for the point by source_id in payload
        scroll_result = rag_engine.client.scroll(
            collection_name=rag_engine.collection,
            scroll_filter=models.Filter(
                must=[
                    models.FieldCondition(
                        key="source_id",
                        match=models.MatchValue(value=nugget_id)
                    )
                ]
            ),
            limit=1
        )

        points, _ = scroll_result

        if not points:
            raise HTTPException(status_code=404, detail=f"Nugget {nugget_id} not found in Qdrant")

        point = points[0]
        point_uuid = point.id

        # 2. Prepare new payload (preserve original structure)
        new_payload = {
            'source': 'rag_nuggets',
            'source_id': nugget_id,
            'title': edit_data.title,
            'content': edit_data.content,
            'keywords': ' '.join(edit_data.keywords) if isinstance(edit_data.keywords, list) else edit_data.keywords,
            'language': edit_data.language,
            'type': point.payload.get('type', 'general'),
            'tags': point.payload.get('tags', []),
            'archetype_filter': point.payload.get('archetype_filter', [])
        }

        # 3. Recalculate embedding (content changed) - ASYNC to avoid blocking event loop
        text_to_embed = f"{edit_data.title} {edit_data.content} {new_payload['keywords']}"
        loop = asyncio.get_event_loop()
        new_embedding = await loop.run_in_executor(None, rag_engine._get_embedding, text_to_embed)

        # 4. Update point in Qdrant
        rag_engine.client.upsert(
            collection_name=rag_engine.collection,
            points=[
                models.PointStruct(
                    id=point_uuid,
                    vector=new_embedding,
                    payload=new_payload
                )
            ]
        )

        logger.info(f"[ADMIN] Nugget {nugget_id} updated successfully in Qdrant (UUID: {point_uuid})")

        return {
            "status": "success",
            "message": f"Nugget {nugget_id} updated successfully",
            "data": {
                "id": nugget_id,
                "uuid": str(point_uuid),
                "title": edit_data.title,
                "embedding_recalculated": True
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[ADMIN] ERROR updating nugget {nugget_id}: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Failed to update nugget: {str(e)}")

@app.post("/api/admin/golden-standards/add")
async def add_golden_standard(standard: GoldenStandardAdd):
    """
    Add golden standard response to both JSON file and Qdrant.

    Steps:
    1. Load existing golden_standards_final.json
    2. Add new standard with unique ID
    3. Save back to JSON file
    4. Add to Qdrant for semantic search
    """
    try:
        import uuid as uuid_lib
        from pathlib import Path

        # 1. Load existing standards from JSON
        json_path = Path(__file__).parent.parent / "dane" / "golden_standards_final.json"

        if json_path.exists():
            with open(json_path, 'r', encoding='utf-8') as f:
                standards = json.load(f)
        else:
            standards = []

        # 2. Create new standard with unique ID
        new_id = f"GS-{str(uuid_lib.uuid4())[:8].upper()}"
        new_standard = {
            "id": new_id,
            "trigger_context": standard.trigger_context,
            "golden_response": standard.golden_response,
            "category": standard.category,
            "language": standard.language,
            "tags": [],
            "created_at": int(time.time() * 1000)
        }

        # 3. Add to list and save to JSON
        standards.append(new_standard)

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(standards, f, ensure_ascii=False, indent=2)

        logger.info(f"[ADMIN] Golden standard {new_id} saved to JSON ({len(standards)} total)")

        # 4. Add to Qdrant (if available)
        if rag_engine.client and rag_engine.model:
            try:
                # Combine trigger + response for embedding
                text_to_embed = f"{standard.trigger_context} {standard.golden_response}"
                embedding = rag_engine._get_embedding(text_to_embed)

                # Prepare payload
                payload = {
                    'source': 'golden_standards',
                    'source_id': new_id,
                    'trigger_context': standard.trigger_context,
                    'golden_response': standard.golden_response,
                    'tags': [],
                    'category': standard.category,
                    'language': standard.language
                }

                # Generate UUID for Qdrant point
                point_uuid = str(uuid_lib.uuid4())

                # Insert to Qdrant
                rag_engine.client.upsert(
                    collection_name=rag_engine.collection,
                    points=[
                        models.PointStruct(
                            id=point_uuid,
                            vector=embedding,
                            payload=payload
                        )
                    ]
                )

                logger.info(f"[ADMIN] Golden standard {new_id} added to Qdrant (UUID: {point_uuid})")

                return {
                    "status": "success",
                    "message": f"Golden standard {new_id} added successfully",
                    "data": {
                        "id": new_id,
                        "uuid": point_uuid,
                        "saved_to_json": True,
                        "saved_to_qdrant": True,
                        "total_standards": len(standards)
                    }
                }

            except Exception as qdrant_error:
                logger.warning(f"[ADMIN] Qdrant insert failed for {new_id}: {qdrant_error}")
                # Still return success since JSON save succeeded
                return {
                    "status": "success",
                    "message": f"Golden standard {new_id} added to JSON (Qdrant failed)",
                    "data": {
                        "id": new_id,
                        "saved_to_json": True,
                        "saved_to_qdrant": False,
                        "total_standards": len(standards),
                        "warning": str(qdrant_error)
                    }
                }
        else:
            # RAG engine not available - JSON only
            return {
                "status": "success",
                "message": f"Golden standard {new_id} added to JSON (Qdrant not available)",
                "data": {
                    "id": new_id,
                    "saved_to_json": True,
                    "saved_to_qdrant": False,
                    "total_standards": len(standards)
                }
            }

    except Exception as e:
        logger.error(f"[ADMIN] ERROR adding golden standard: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Failed to add golden standard: {str(e)}")


# === FEEDBACK LOOP (AI DOJO) ENDPOINTS ===

@app.post("/api/feedback")
async def submit_feedback(feedback: FeedbackSubmit, db: AsyncSession = Depends(get_db)):
    """
    AI DOJO: Submit expert feedback on AI responses.
    Used for continuous learning and model improvement.
    """
    import uuid
    
    feedback_id = str(uuid.uuid4())
    
    new_feedback = FeedbackLog(
        id=feedback_id,
        session_id=feedback.session_id,
        module_name=feedback.module_name,
        rating=feedback.rating,
        user_input_snapshot=feedback.user_input_snapshot,
        ai_output_snapshot=feedback.ai_output_snapshot,
        expert_comment=feedback.expert_comment,
        message_id=feedback.message_id
    )
    
    db.add(new_feedback)
    await db.commit()
    
    rating_emoji = "üëç" if feedback.rating else "üëé"
    print(f"[DOJO] {rating_emoji} New expert feedback received for {feedback.module_name}")
    if feedback.expert_comment:
        print(f"[DOJO] Comment: {feedback.expert_comment[:100]}...")
    
    return {
        "status": "success",
        "id": feedback_id,
        "message": f"Feedback saved for {feedback.module_name}"
    }


@app.get("/api/feedback")
async def list_feedback(
    module_name: Optional[str] = None,
    rating: Optional[bool] = None,
    limit: int = 50,
    db: AsyncSession = Depends(get_db)
):
    """
    AI DOJO: List feedback entries for training dashboard.
    Optionally filter by module_name or rating.
    """
    from sqlalchemy import desc
    
    query = select(FeedbackLog)
    
    if module_name:
        query = query.where(FeedbackLog.module_name == module_name)
    if rating is not None:
        query = query.where(FeedbackLog.rating == rating)
    
    query = query.order_by(desc(FeedbackLog.timestamp)).limit(limit)
    
    result = await db.execute(query)
    feedback_items = result.scalars().all()
    
    return {
        "total": len(feedback_items),
        "items": [
            {
                "id": f.id,
                "session_id": f.session_id,
                "module_name": f.module_name,
                "rating": f.rating,
                "user_input_snapshot": f.user_input_snapshot,
                "ai_output_snapshot": f.ai_output_snapshot,
                "expert_comment": f.expert_comment,
                "message_id": f.message_id,
                "timestamp": f.timestamp
            }
            for f in feedback_items
        ]
    }


@app.get("/api/feedback/stats")
async def feedback_stats(db: AsyncSession = Depends(get_db)):
    """
    AI DOJO: Get feedback statistics for the dashboard.
    """
    from sqlalchemy import func

    # Total counts
    total_query = select(func.count(FeedbackLog.id))
    total_result = await db.execute(total_query)
    total_count = total_result.scalar() or 0

    # Positive count
    positive_query = select(func.count(FeedbackLog.id)).where(FeedbackLog.rating == True)
    positive_result = await db.execute(positive_query)
    positive_count = positive_result.scalar() or 0

    # Negative count
    negative_count = total_count - positive_count

    # Count by module
    module_query = select(
        FeedbackLog.module_name,
        func.count(FeedbackLog.id).label('count')
    ).group_by(FeedbackLog.module_name)
    module_result = await db.execute(module_query)
    module_counts = {row.module_name: row.count for row in module_result}

    return {
        "total": total_count,
        "positive": positive_count,
        "negative": negative_count,
        "approval_rate": round((positive_count / total_count * 100) if total_count > 0 else 0, 1),
        "by_module": module_counts
    }


# === GOTHAM ENDPOINTS (NEW in v4.0) ===

@app.post("/api/gotham/score")
async def calculate_gotham_score(request: GothamScoreRequest):
    """
    GOTHAM: Calculate Burning House Score

    Returns financial urgency analysis for switching to Tesla.
    """
    try:
        full_context = GothamIntelligence.get_full_context(
            monthly_fuel_cost=request.monthly_fuel_cost,
            current_car_value=request.current_car_value,
            annual_tax=request.annual_tax,
            has_family_card=request.has_family_card,
            region=request.region
        )

        print(f"[GOTHAM] Score calculated - Urgency: {full_context['urgency_level']}")

        return {
            "status": "success",
            "data": full_context
        }

    except Exception as e:
        print(f"[GOTHAM] ERROR - {e}")
        import traceback
        traceback.print_exc()

        raise HTTPException(status_code=500, detail=f"GOTHAM calculation failed: {str(e)}")


@app.get("/api/gotham/market/{region}")
async def get_market_data(region: str):
    """
    GOTHAM: Get CEPiK market data for a region

    Returns regional EV registration statistics.
    """
    try:
        from backend.gotham_module import CEPiKConnector

        data = CEPiKConnector.get_regional_data(region)
        context = CEPiKConnector.get_market_context(region)

        if not data:
            raise HTTPException(status_code=404, detail=f"No data for region: {region}")

        return {
            "status": "success",
            "data": data.dict(),
            "context": context
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"[GOTHAM] ERROR - {e}")
        raise HTTPException(status_code=500, detail=f"Market data fetch failed: {str(e)}")


@app.put("/api/admin/gotham/market")
async def update_market_data(update: GothamMarketUpdate):
    """
    ADMIN: Update GOTHAM market data for a region

    Allows manual updates when real CEPiK API is unavailable.
    Data is saved to JSON and persists across restarts.
    """
    try:
        updated_data = CEPiKConnector.update_market_data(
            region=update.region,
            total_ev_registrations=update.total_ev_registrations,
            growth_rate=update.growth_rate,
            top_brand=update.top_brand,
            trend=update.trend
        )

        logger.info(f"[ADMIN] Market data updated for {update.region}: {update.total_ev_registrations} EVs")

        return {
            "status": "success",
            "message": f"Market data for {update.region} updated successfully",
            "data": updated_data.dict()
        }

    except Exception as e:
        logger.error(f"[ADMIN] ERROR updating market data: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Failed to update market data: {str(e)}")


@app.post("/api/admin/dojo/run-refine")
async def run_dojo_refinement():
    """
    ADMIN: Run DOJO-REFINER - AI Auto-Improvement Engine

    V4.0 FEATURE: Analyzes negative expert feedback and generates improvement suggestions.

    Workflow:
    1. Scans FeedbackLog for unprocessed negative feedback (rating=False, processed=False)
    2. Groups by module_name (fast_path, slow_path_m1_dna, etc.)
    3. For modules with 3+ negative feedback, uses AI to generate fixes
    4. Saves suggestions to dane/suggested_fixes.json (Human-in-the-Loop)
    5. Marks feedback as processed

    Returns:
        {
            "processed_modules": ["fast_path", "slow_path_m1_dna"],
            "total_feedback_processed": 12,
            "fixes_generated": 2,
            "suggestions_saved_to": "dane/suggested_fixes.json",
            "timestamp": 1234567890
        }
    """
    try:
        logger.info("[ADMIN] üß† DOJO-REFINER requested by admin")

        # Run refinement
        result = await dojo_refiner.scan_and_refine()

        logger.info(f"[ADMIN] ‚úÖ DOJO-REFINER completed: {result['fixes_generated']} fix(es) generated")

        return {
            "status": "success",
            "message": "DOJO-REFINER completed successfully",
            **result
        }

    except Exception as e:
        logger.error(f"[ADMIN] ‚ùå DOJO-REFINER ERROR: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"DOJO-REFINER failed: {str(e)}")


@app.get("/api/v1/gotham/market-overview")
async def get_market_overview(region: str = "≈öLƒÑSKIE"):
    """
    GOTHAM: Market Overview - Lead Sniper Data

    Returns real-time data about expiring leases (potential leads) from CEPiK API.
    This powers the "Lead Sniper" widget on the dashboard.

    Args:
        region: Voivodeship name (default: ≈öLƒÑSKIE)

    Returns:
        {
            "total_expiring_leases": 824,
            "competitor_breakdown": {"BMW": 245, "MERCEDES-BENZ": 312, ...},
            "opportunity_score": 85,
            "urgency_level": "HIGH",
            "insight": "824 premium car leases expiring - strong sales opportunity",
            "region": "≈öLƒÑSKIE",
            "last_updated": "2026-01-04T10:30:00"
        }
    """
    try:
        from datetime import datetime

        print(f"[GOTHAM] üéØ Fetching market overview for {region}...")

        # Get opportunity score (includes leasing expiry data)
        opportunity_data = CEPiKConnector.get_opportunity_score(region=region)

        # Add metadata
        opportunity_data["region"] = region
        opportunity_data["last_updated"] = datetime.now().isoformat()

        print(f"[GOTHAM] Market Overview: {opportunity_data['total_expiring_leases']} expiring leases, Score: {opportunity_data['opportunity_score']}")

        return opportunity_data

    except Exception as e:
        logger.error(f"[GOTHAM] ERROR fetching market overview: {e}")
        import traceback
        traceback.print_exc()

        # Return safe fallback data
        from datetime import datetime
        return {
            "total_expiring_leases": 0,
            "competitor_breakdown": {},
            "opportunity_score": 0,
            "urgency_level": "UNKNOWN",
            "insight": "Data temporarily unavailable",
            "region": region,
            "last_updated": datetime.now().isoformat(),
            "error": str(e)
        }


# === ASSET SNIPER v5.0 ENDPOINTS (UnifiedPipeline + Async Jobs) ===

@app.post("/api/sniper/upload")
async def sniper_upload_csv(
    file: UploadFile = File(...),
    enable_deep_enrichment: bool = False,
    background_tasks: BackgroundTasks = None
):
    """
    ASSET SNIPER v5.0: Upload CSV and start async processing job.

    Returns job_id immediately - use /api/sniper/status/{id} to check progress.

    Pipeline:
    - UnifiedPipeline with Golden City Set M¬≤ pricing
    - BigDecoder Full Bridge integration
    - GOTHAM Engine (wealth, tax, chargers)
    - Scoring Matrix (Tier S-E classification)

    Args:
        file: CSV file with CEIDG leads
        enable_deep_enrichment: Whether to run BigDecoder AI profiling

    Returns:
        {"job_id": "...", "status": "pending", "message": "..."}
    """
    import io

    try:
        import pandas as pd
    except ImportError:
        raise HTTPException(
            status_code=500,
            detail="pandas is not installed. Run: pip install pandas"
        )

    # Validate file type - support both CSV and XML
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in ['.csv', '.xml']:
        raise HTTPException(
            status_code=400,
            detail="Invalid file type. Please upload a CSV or XML file."
        )

    logger.info(f"[SNIPER v5.0] üì• Received file: {file.filename} (Type: {file_ext})")
    logger.info(f"[SNIPER v5.0] Deep enrichment: {enable_deep_enrichment}")

    # Save uploaded file to temporary location for streaming processing
    import tempfile
    temp_file = tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix=file_ext)
    temp_path = temp_file.name

    try:
        # Stream file to disk in chunks (avoid loading into memory)
        chunk_size = 1024 * 1024  # 1MB chunks
        while True:
            chunk = await file.read(chunk_size)
            if not chunk:
                break
            temp_file.write(chunk)
        temp_file.close()
        logger.info(f"[SNIPER] File saved to temporary location: {temp_path}")
    except Exception as e:
        temp_file.close()
        Path(temp_path).unlink(missing_ok=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to save uploaded file: {str(e)}"
        )

    # Generate job_id
    job_id = f"JOB-{str(uuid.uuid4())[:8].upper()}"

    # Initialize job status (we'll update total_rows during processing)
    SNIPER_JOBS[job_id] = {
        "status": SniperJobStatus.PENDING,
        "progress": 0,
        "filename": file.filename,
        "file_path": temp_path,
        "file_type": file_ext,
        "total_rows": 0,  # Will be updated during processing
        "enable_deep": enable_deep_enrichment,
        "created_at": int(time.time() * 1000),
        "result_csv": None,
        "stats": None,
        "error": None
    }

    # Start background processing with file path instead of DataFrame
    if background_tasks:
        background_tasks.add_task(process_file_job, job_id, temp_path, file_ext, enable_deep_enrichment)
    else:
        # Fallback: create asyncio task
        asyncio.create_task(process_file_job(job_id, temp_path, file_ext, enable_deep_enrichment))

    logger.info(f"[SNIPER v5.0] Created job {job_id} for streaming processing")

    return {
        "job_id": job_id,
        "status": SniperJobStatus.PENDING,
        "message": f"Job created. Processing file with {'deep' if enable_deep_enrichment else 'local'} enrichment using streaming.",
        "file_type": file_ext
    }


@app.get("/api/sniper/status/{job_id}")
async def sniper_job_status(job_id: str):
    """
    ASSET SNIPER v5.0: Check job processing status.

    Args:
        job_id: Job ID returned from /api/sniper/upload

    Returns:
        {
            "job_id": "JOB-XXXX",
            "status": "processing|completed|failed",
            "progress": 0-100,
            "stats": {...} (if completed)
        }
    """
    if job_id not in SNIPER_JOBS:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

    job = SNIPER_JOBS[job_id]

    response = {
        "job_id": job_id,
        "status": job["status"],
        "progress": job["progress"],
        "filename": job.get("filename"),
        "total_rows": job.get("total_rows"),
        "enable_deep": job.get("enable_deep"),
        "created_at": job.get("created_at")
    }

    if job["status"] == SniperJobStatus.COMPLETED:
        response["stats"] = job.get("stats")
        response["completed_at"] = job.get("completed_at")

    if job["status"] == SniperJobStatus.FAILED:
        response["error"] = job.get("error")

    return response


@app.get("/api/sniper/download/{job_id}")
async def sniper_download_result(job_id: str):
    """
    ASSET SNIPER v5.0: Download enriched CSV result.

    Args:
        job_id: Job ID returned from /api/sniper/upload

    Returns:
        Streaming CSV response with enriched data
    """
    import io
    from datetime import datetime

    if job_id not in SNIPER_JOBS:
        raise HTTPException(status_code=404, detail=f"Job {job_id} not found")

    job = SNIPER_JOBS[job_id]

    if job["status"] != SniperJobStatus.COMPLETED:
        raise HTTPException(
            status_code=400,
            detail=f"Job {job_id} is not completed. Status: {job['status']}"
        )

    if not job.get("result_csv"):
        raise HTTPException(status_code=500, detail="Result CSV not available")

    # Generate filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    enrichment_suffix = "_deep" if job.get("enable_deep") else "_local"
    output_filename = f"sniper_enriched{enrichment_suffix}_{timestamp}.csv"

    # Return streaming response
    return StreamingResponse(
        io.BytesIO(job["result_csv"].encode('utf-8')),
        media_type="text/csv",
        headers={
            "Content-Disposition": f"attachment; filename={output_filename}",
            "X-Sniper-Stats": json.dumps(job.get("stats", {}))
        }
    )


@app.post("/api/sniper/analyze")
async def sniper_analyze_csv(
    file: UploadFile = File(...),
    include_intelligence: bool = False
):
    """
    ASSET SNIPER v5.0: Quick CSV analysis (preview before full processing).

    Uses UnifiedPipeline Level 2 (GOTHAM) for fast results.

    Args:
        file: CSV file with CEIDG leads
        include_intelligence: Include Golden City Set pricing data

    Returns:
        Statistics and sample Tier S leads
    """
    import io

    try:
        import pandas as pd
    except ImportError:
        raise HTTPException(status_code=500, detail="pandas not installed")

    try:
        if not file.filename.endswith('.csv'):
            raise HTTPException(status_code=400, detail="Invalid file type")

        content = await file.read()

        # Parse CSV
        df = None
        for encoding in ['utf-8', 'cp1250', 'iso-8859-2', 'latin1']:
            try:
                df = pd.read_csv(io.BytesIO(content), encoding=encoding, sep=None, engine='python')
                break
            except Exception:
                continue

        if df is None:
            raise HTTPException(status_code=400, detail="Could not parse CSV")

        # Process with UnifiedPipeline (Level 2 GOTHAM - fast)
        pipeline = UnifiedPipeline()
        df_enriched, stats = pipeline.process_sync(df, level=ProcessingLevel.LEVEL_2_GOTHAM)

        # Get sample Tier S leads
        tier_s_df = df_enriched[df_enriched['target_tier'] == Tier.S.value].head(5) if 'target_tier' in df_enriched.columns else pd.DataFrame()

        # Find relevant columns
        name_col = None
        city_col = None

        for col in df_enriched.columns:
            col_lower = col.lower()
            if 'name' in col_lower or 'nazwa' in col_lower:
                name_col = col
            if 'city' in col_lower or 'miasto' in col_lower or 'miejscow' in col_lower:
                city_col = col

        # Helper function to map backend tiers to frontend tiers
        def map_tier_to_frontend(tier: str) -> str:
            tier_mapping = {
                "S": "Tier S",
                "AAA": "Tier A",
                "AA": "Tier A",
                "A": "Tier A",
                "B": "Tier B",
                "C": "Tier C",
                "D": "Tier C",
                "E": "Tier C",
            }
            return tier_mapping.get(tier, "Unknown")

        # Helper to get DNA type from lead_type
        def get_dna_type(lead_type: str, pkd_code: str) -> str:
            # Map lead types to DNA types
            if "ANALYTICAL" in lead_type.upper() or "CFO" in lead_type.upper():
                return "Analytical"
            elif "TECH" in lead_type.upper() or "VISIONARY" in lead_type.upper():
                return "Visionary"
            elif "COST" in lead_type.upper() or "FLEET" in lead_type.upper():
                return "Cost-Driven"
            elif "STATUS" in lead_type.upper() or "AFFLUENT" in lead_type.upper() or "ALPHA" in lead_type.upper():
                return "Status-Seeker"
            elif "PRAGMATIC" in lead_type.upper() or "TIME_POOR" in lead_type.upper():
                return "Pragmatic"
            # Fallback based on PKD
            pkd_dna_map = {
                "6910Z": "Status-Seeker",  # Lawyers
                "6920Z": "Analytical",     # Accountants
                "6201Z": "Visionary",      # IT
                "8621Z": "Pragmatic",      # Doctors
                "4941Z": "Cost-Driven",    # Transport
            }
            return pkd_dna_map.get(pkd_code, "Unknown")

        sample_leads = []
        for _, row in tier_s_df.iterrows():
            company = str(row.get(name_col, "Unknown")) if name_col else "Unknown"
            city = str(row.get(city_col, "")) if city_col else ""

            # Get PKD code for industry name
            pkd_code = str(row.get('pkd_clean', '') or row.get('GlownyKodPkd', '') or row.get('PkdGlowny', ''))

            # Get industry name from PKD profiles
            from asset_sniper.config import PKD_PROFILES
            pkd_profile = PKD_PROFILES.get(pkd_code, PKD_PROFILES.get("DEFAULT", {}))
            industry_name = pkd_profile.get('full_name', 'Dzia≈Çalno≈õƒá gospodarcza')

            # Build lead data matching frontend SampleLead interface
            lead_data = {
                "company": company,
                "tier_score": int(row.get('total_score', 0)),
                "wealth_tier": str(row.get('wealth_tier', 'STANDARD')),
                "leasing_cycle": str(row.get('leasing_cycle', 'UNKNOWN')),
                "next_action": str(row.get('next_action', '')),
                "industry_name": industry_name,
            }

            # Add intelligence fields
            if include_intelligence:
                # Tax saving from GOTHAM engine
                tax_saving = float(row.get('Potential_Savings_PLN', 0) or row.get('tax_benefit_total_first_year', 0) or 0)
                lead_data["estimated_tax_saving"] = tax_saving if tax_saving > 0 else 14250.0  # Default estimate

                # Charger distance
                charger_km = float(row.get('charger_distance_km', 0) or 0)
                lead_data["estimated_charger_km"] = charger_km if charger_km > 0 else 5.0  # Default estimate

                # DNA type
                lead_type = str(row.get('lead_type', ''))
                lead_data["estimated_dna_type"] = get_dna_type(lead_type, pkd_code)

                # Market urgency (based on wealth score and leasing cycle)
                wealth_score = int(row.get('wealth_score', 5))
                leasing_prop = float(row.get('leasing_propensity', 0.5))
                urgency = min(100, int((wealth_score * 6) + (leasing_prop * 40)))
                lead_data["market_urgency"] = urgency

            sample_leads.append(lead_data)

        # Map tier distribution to frontend format
        frontend_tier_dist = {}
        for tier, count in stats.tier_counts.items():
            frontend_tier = map_tier_to_frontend(tier)
            frontend_tier_dist[frontend_tier] = frontend_tier_dist.get(frontend_tier, 0) + count

        return {
            "status": "success",
            "filename": file.filename,
            "total_rows": stats.total_rows,
            "processed_rows": stats.scored_rows,  # Frontend expects processed_rows
            "cleaned_rows": stats.cleaned_rows,
            "scored_rows": stats.scored_rows,
            "tier_distribution": frontend_tier_dist,  # Use mapped tier names
            "top_regions": stats.top_cities,  # Frontend expects top_regions
            "top_industries": stats.top_pkd_industries,
            "top_cities": stats.top_cities,
            "avg_wealth_score": round(stats.avg_wealth_score, 1),
            "avg_total_score": round(stats.avg_total_score, 1),
            "processing_time_ms": stats.processing_time_ms,
            "sample_tier_s": sample_leads,
            "intelligence_included": include_intelligence
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[SNIPER] ERROR analyzing CSV: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/sniper/charger-check/{city}")
async def sniper_charger_check(city: str):
    """
    ASSET SNIPER: Check charger infrastructure for a city
    
    Returns charging infrastructure data for EV sales pitch.
    """
    try:
        data = SniperGateway.check_charger_infrastructure(city=city)
        return {"status": "success", "data": data}
    except Exception as e:
        logger.error(f"[SNIPER] ERROR checking chargers: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/sniper/tax-potential")
async def sniper_tax_potential(
    pkd_code: str = None,
    legal_form: str = None,
    estimated_annual_km: int = 25000
):
    """
    ASSET SNIPER: Calculate tax savings potential for a lead
    
    Returns detailed tax benefit analysis for EV switch.
    """
    try:
        data = SniperGateway.calculate_tax_potential(
            pkd_code=pkd_code,
            legal_form=legal_form,
            estimated_annual_km=estimated_annual_km
        )
        return {"status": "success", "data": data}
    except Exception as e:
        logger.error(f"[SNIPER] ERROR calculating tax: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# === SESSION ENDPOINTS ===

@app.post("/api/sessions")
async def create_session(db: AsyncSession = Depends(get_db)):
    """Create new session."""
    import uuid
    session_id = f"S-{str(uuid.uuid4())[:8].upper()}"
    
    new_session = DBSession(id=session_id, created_at=int(time.time() * 1000))
    
    # Initialize empty analysis state
    initial_analysis = DBAnalysisState(
        session_id=session_id,
        data={
             "m1_dna": { "summary": "Initializing...", "mainMotivation": "Unknown", "communicationStyle": "Analytical" },
             "m2_indicators": { "purchaseTemperature": 0, "churnRisk": "Low", "funDriveRisk": "Low" },
             "m3_psychometrics": {
                "disc": { "dominance": 50, "influence": 50, "steadiness": 50, "compliance": 50 },
                "bigFive": { "openness": 50, "conscientiousness": 50, "extraversion": 50, "agreeableness": 50, "neuroticism": 50 },
                "schwartz": { "opennessToChange": 50, "selfEnhancement": 50, "conservation": 50, "selfTranscendence": 50 }
              },
             "m4_motivation": { "keyInsights": [], "teslaHooks": [] },
             "m5_predictions": { "scenarios": [], "estimatedTimeline": "Unknown" },
             "m6_playbook": { "suggestedTactics": [], "ssr": [] },
             "m7_decision": { "decisionMaker": "Unknown", "influencers": [], "criticalPath": "Unknown" },
             "journeyStageAnalysis": { "currentStage": "DISCOVERY", "confidence": 0, "reasoning": "Initializing..." },
             "isAnalyzing": False,
             "lastUpdated": int(time.time() * 1000)
        }
    )
    
    db.add(new_session)
    db.add(initial_analysis)
    await db.commit()
    
    return {"id": session_id}

@app.get("/api/sessions/{session_id}")
async def get_session(session_id: str, db: AsyncSession = Depends(get_db)):
    """Get session with messages and analysis."""
    stmt = select(DBSession).where(DBSession.id == session_id).options(
        selectinload(DBSession.messages),
        selectinload(DBSession.analysis_state)
    )
    result = await db.execute(stmt)
    session = result.scalar_one_or_none()
    
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    
    return {
        "id": session.id,
        "createdAt": session.created_at,
        "status": session.status,
        "outcome": session.outcome,
        "journeyStage": session.journey_stage,
        "messages": [
            {
                "id": m.id,
                "role": m.role,
                "content": m.content,
                "timestamp": m.timestamp,
                "confidence": m.confidence,
                "confidenceReason": m.confidence_reason,
                "clientStyle": m.client_style,
                "contextNeeds": m.context_needs,
                "suggestedActions": m.suggested_actions,
                "feedback": m.feedback,
                "feedbackDetails": m.feedback_details
            } for m in session.messages
        ],
        "analysisState": session.analysis_state.data if session.analysis_state else {}
    }

# === BACKGROUND TASKS (SLOW PATH) ===

async def run_slow_analysis_safe(websocket_manager, session_id: str, history: list, rag_context: str, journey_stage: str, language: str = "PL"):
    """
    ULTRA V4.0: Safe Guard for Slow Path (Background) with Queue Management

    V4.0 FIX: Handles SystemBusyException and notifies user via WebSocket
    """
    try:
        print(f"[SLOW PATH] Starting analysis for {session_id} (Language: {language})...")

        # 1. Run deep analysis (with queue-based concurrency control)
        analysis_result = await analysis_engine.run_deep_analysis(
            session_id=session_id,
            chat_history=history,
            language=language
        )

        if not analysis_result:
            print(f"[SLOW PATH] Analysis skipped or failed safely.")
            return
        
        # 2. Save to DB
        async with AsyncSessionLocal() as db:
            stmt = select(DBAnalysisState).where(DBAnalysisState.session_id == session_id)
            result = await db.execute(stmt)
            db_analysis = result.scalar_one_or_none()
            
            if db_analysis:
                current_data = db_analysis.data
                current_data.update(analysis_result)
                current_data["isAnalyzing"] = False
                current_data["lastUpdated"] = int(time.time() * 1000)
                
                db_analysis.data = dict(current_data)
                await db.commit()
                
                # 3a. AUTO-UPDATE JOURNEY STAGE if AI is confident
                if analysis_result.get('journeyStageAnalysis'):
                    stage_analysis = analysis_result['journeyStageAnalysis']
                    suggested_stage = stage_analysis.get('currentStage', '')
                    stage_confidence = stage_analysis.get('confidence', 0)
                    
                    # If AI is 75%+ confident AND stage changed
                    if stage_confidence >= 75 and suggested_stage:
                        stmt_sess = select(DBSession).where(DBSession.id == session_id)
                        res_sess = await db.execute(stmt_sess)
                        session_obj = res_sess.scalar_one_or_none()
                        
                        if session_obj and session_obj.journey_stage != suggested_stage:
                            old_stage = session_obj.journey_stage
                            session_obj.journey_stage = suggested_stage
                            await db.commit()
                            print(f"[AUTO-STAGE] {session_id}: {old_stage} -> {suggested_stage} (Confidence: {stage_confidence}%)")
                        else:
                            print(f"[AUTO-STAGE] Stage unchanged: {suggested_stage} (Confidence: {stage_confidence}%)")
                
                # 3b. Send WebSocket Update to Frontend
                print(f"[SLOW PATH] Sending update to UI: {list(current_data.keys())}")
                await websocket_manager.broadcast({
                    "type": "analysis_update",
                    "session_id": session_id,
                    "data": current_data
                })
                
                print(f"[SLOW PATH] OK - Analysis saved and broadcasted for {session_id}")

    except SystemBusyException as busy_err:
        # V4.0 FIX: Handle queue timeout - notify user instead of silent failure
        print(f"[SLOW PATH] SYSTEM BUSY - {busy_err.message}")
        try:
            await websocket_manager.broadcast({
                "type": "system_busy",
                "session_id": session_id,
                "message": busy_err.message,
                "code": "ANALYSIS_QUEUE_FULL",
                "retry_after": int(busy_err.timeout)
            })
        except Exception as broadcast_err:
            print(f"[SLOW PATH] ERROR - Failed to notify user of system busy: {broadcast_err}")

    except Exception as e:
        print(f"[SLOW PATH] ERROR - {e}")
        import traceback
        traceback.print_exc()

        # Notify frontend of failure
        try:
            await websocket_manager.broadcast({
                "type": "analysis_error",
                "session_id": session_id,
                "error": str(e)
            })
        except Exception as broadcast_err:
            print(f"[SLOW PATH] ERROR - Broadcast notification failed: {broadcast_err}")


# === WEBSOCKET MANAGER (IMPROVED WITH MESSAGE QUEUE) ===

class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        # CRITICAL FIX #3: Store task references to prevent GC
        self.active_tasks: Dict[str, asyncio.Task] = {}
        # CRITICAL FIX #6: Queue messages when no active connections
        self.pending_messages: Dict[str, List[dict]] = {}

    async def connect(self, websocket: WebSocket, client_id: str):
        await websocket.accept()
        self.active_connections.append(websocket)
        print(f"[WS] OK - Client connected: {client_id} (Total: {len(self.active_connections)})")
        
        # FIX #6: Send queued messages on connect
        if client_id in self.pending_messages:
            print(f"[WS] Sending {len(self.pending_messages[client_id])} queued messages to {client_id}")
            for msg in self.pending_messages[client_id]:
                try:
                    await websocket.send_json(msg)
                except Exception as e:
                    print(f"[WS] ERROR - Failed to send queued message: {e}")
            del self.pending_messages[client_id]

    def disconnect(self, websocket: WebSocket, client_id: str):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
        print(f"üîå [WS] Client disconnected: {client_id} (Remaining: {len(self.active_connections)})")

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def broadcast(self, message: dict):
        """
        FIX #6: Queue messages if no active connections
        """
        session_id = message.get("session_id")
        
        if not self.active_connections:
            # Queue for later delivery
            if session_id:
                if session_id not in self.pending_messages:
                    self.pending_messages[session_id] = []
                self.pending_messages[session_id].append(message)
                print(f"[WS] WARN - No active connections - queued message for {session_id} (Type: {message.get('type')})")
            return
        
        # Broadcast to all
        dead_connections = []
        for connection in self.active_connections:
            try:
                await connection.send_json(message)
            except Exception as e:
                print(f"[WS] ERROR - Send failed to connection: {e}")
                dead_connections.append(connection)
        
        # Clean up dead connections
        for conn in dead_connections:
            if conn in self.active_connections:
                self.active_connections.remove(conn)

manager = ConnectionManager()

# === WEBSOCKET ENDPOINT (WITH GC FIX) ===

@app.websocket("/ws/chat/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    """
    ULTRA V3.1 LITE: Main Orchestrator
    """
    await manager.connect(websocket, session_id)
    print(f"[WS] Client connected: {session_id}")
    
    try:
        while True:
            # 1. Receive
            data = await websocket.receive_text()
            
            # Parse (handling both JSON and raw strings)
            try:
                payload = json.loads(data)
                content = payload.get("content", data)
                # Extract language from payload (default to PL for backward compatibility)
                message_language = payload.get("language", "PL")
            except:
                content = data
                message_language = "PL"
            
            print(f"[WS] Received: {content[:50]}... (Language: {message_language})")
            
            # Send "Processing" ack
            await websocket.send_json({"type": "processing", "data": {"status": "started"}})

            # 2. Get/Create Session & Save User Message
            current_stage = "DISCOVERY"
            history = []
            
            async with AsyncSessionLocal() as db:
                # Find or Create Session
                stmt = select(DBSession).where(DBSession.id == session_id).options(selectinload(DBSession.messages))
                result = await db.execute(stmt)
                session = result.scalar_one_or_none()
                
                if not session:
                    session = DBSession(id=session_id, journey_stage="DISCOVERY", created_at=int(time.time()*1000))
                    db.add(session)
                    db.add(DBAnalysisState(session_id=session_id, data={"isAnalyzing": False}))
                    await db.commit()
                
                current_stage = session.journey_stage
                
                # Save User Message
                user_msg = DBMessage(
                    id=str(int(time.time()*1000)),
                    session_id=session_id,
                    role="user",
                    content=content,
                    timestamp=int(time.time() * 1000)
                )
                db.add(user_msg)
                await db.commit()
                
                # Refresh history
                await db.refresh(session, ["messages"])
                history = [{"role": m.role, "content": m.content} for m in session.messages]

            # 3. RAG Search (Context)
            rag_context_str = ""
            try:
                rag_results = await rag_engine.search_async(content, limit=3)
                if rag_results:
                    chunks = []
                    for r in rag_results:
                        chunks.append(f"[{r.get('title','Info')}]: {r.get('content','')}")
                    rag_context_str = "\n".join(chunks)
                    print(f"[RAG] Found {len(rag_results)} nuggets")
            except Exception as e:
                print(f"[RAG] Warning: {e}")

            # 3.5. GOTHAM Intelligence (NEW in v4.0) - LIVE Smart Detection
            gotham_context = None

            # EXPANDED keyword detection (PL + EN)
            financial_keywords = [
                # Polish
                "paliwo", "benzyna", "diesel", "lpg", "olej", "tankowanie",
                "oszczƒôdno≈õci", "oszczƒôdziƒá", "taniej", "dro≈ºej",
                "koszt", "koszty", "wydatek", "wydatki", "p≈Çacƒô", "p≈Çaci",
                "podatek", "op≈Çata", "rata", "leasing",
                "spalanie", "pali", "zu≈ºycie", "zu≈ºywa",
                "tco", "ca≈Çkowity koszt", "utrzymanie",
                # English
                "fuel", "gas", "petrol", "gasoline", "diesel", "lpg",
                "savings", "save", "cheaper", "expensive",
                "cost", "costs", "expense", "expenses", "paying", "payment",
                "tax", "fee", "lease", "leasing",
                "consumption", "burns", "mpg",
                "tco", "total cost", "maintenance"
            ]

            # Check if conversation contains financial keywords (last 5 messages)
            recent_messages = " ".join([msg['content'].lower() for msg in history[-5:]])
            intent_detected = any(keyword in recent_messages for keyword in financial_keywords)

            if intent_detected:
                try:
                    print(f"[GOTHAM] üî• Financial intent detected! Triggering Burning House calculation...")

                    # Generate GOTHAM context with live fuel prices
                    # TODO: In production, extract these values from session metadata or client profile
                    gotham_context = GothamIntelligence.get_full_context(
                        monthly_fuel_cost=1200,  # Default: 1,200 PLN/month
                        current_car_value=80_000,  # Default: 80k PLN
                        annual_tax=225_000,  # Default: high emission tax (alternative: 0)
                        has_family_card=False,  # Default: no family card
                        region="≈öLƒÑSKIE"  # Default: ≈ölƒÖskie region
                    )
                    print(f"[GOTHAM] Context generated - Urgency: {gotham_context['urgency_level']}")
                    print(f"[GOTHAM] Annual savings: {gotham_context['burning_house_score']['annual_savings']:,.0f} PLN")

                    # BROADCAST to Frontend immediately (before AI response)
                    # This triggers the red "Burning House" alert on Dashboard
                    await websocket.send_json({
                        "type": "gotham_update",
                        "data": gotham_context
                    })
                    print(f"[GOTHAM] ‚úÖ Broadcasted to Frontend - Widget should appear NOW!")

                except Exception as e:
                    print(f"[GOTHAM] WARNING - Calculation failed: {e}")
                    import traceback
                    traceback.print_exc()
                    gotham_context = None

            # 4. FAST PATH (AI Core)
            try:
                fast_response = await ai_core.fast_path_secure(
                    history=history,
                    rag_context=rag_context_str,
                    stage=current_stage,
                    language=message_language,
                    gotham_context=gotham_context
                )
                # Log if fallback was used
                if "FALLBACK" in fast_response.confidence_reason:
                    print(f"[FAST PATH] ‚ö†Ô∏è FALLBACK MODE: {fast_response.confidence_reason}")
            except Exception as e:
                print(f"[FAST PATH] ERROR - {e}")
                import traceback
                traceback.print_exc()
                fast_response = create_emergency_response(message_language)
                print(f"[FAST PATH] ‚ö†Ô∏è EMERGENCY_FALLBACK triggered due to exception")

            # 5. SAVE AI RESPONSE & MAP NEW FIELDS
            async with AsyncSessionLocal() as db:
                ai_msg = DBMessage(
                    id=str(int(time.time()*1000) + 1),
                    session_id=session_id,
                    role="ai",
                    content=fast_response.response,
                    timestamp=int(time.time() * 1000),
                    confidence=fast_response.confidence,
                    confidence_reason=fast_response.confidence_reason,
                    client_style="Analytical",
                    context_needs=fast_response.tactical_next_steps,
                    suggested_actions=fast_response.knowledge_gaps
                )
                db.add(ai_msg)
                await db.commit()

            # 6. SEND TO FRONTEND
            await websocket.send_json({
                "type": "fast_response",
                "data":  {
                    "id": ai_msg.id,
                    "role": "ai",
                    "content": ai_msg.content,
                    "timestamp": ai_msg.timestamp,
                    "confidence": ai_msg.confidence,
                    "confidenceReason": ai_msg.confidence_reason,
                    "clientStyle": ai_msg.client_style,
                    "contextNeeds": ai_msg.context_needs,
                    "suggestedActions": ai_msg.suggested_actions
                }
            })
            print(f"[FAST PATH] OK - Sent response to {session_id}")

            # 7. TRIGGER SLOW PATH (Background) - FIX #3: STORE TASK REFERENCE
            await websocket.send_json({
                "type": "analysis_status",
                "data": {"status": "analyzing", "session_id": session_id}
            })
            
            task = asyncio.create_task(
                run_slow_analysis_safe(
                    manager, 
                    session_id, 
                    history, 
                    rag_context_str, 
                    current_stage,
                    message_language
                )
            )
            
            # CRITICAL FIX #3: Store reference to prevent GC
            manager.active_tasks[session_id] = task
            print(f"[TASK] Stored reference for {session_id} (Active tasks: {len(manager.active_tasks)})")

    except WebSocketDisconnect:
        manager.disconnect(websocket, session_id)
        print(f"üîå Client disconnected: {session_id}")
    except Exception as e:
        print(f"[WS] CRITICAL ERROR - {e}")
        import traceback
        traceback.print_exc()
        try:
            await websocket.close()
        except:
            pass
